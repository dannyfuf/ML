# Conformidad Ética
El trabajo fue realizado de manera original por Hans Fluhmann y Danny Fuentes. Hans Fluhmann se encagó de seleccionar los datasets, diseñar los experimentos y procesar 2 de los 3 datasets. Danny Fuentes se encargó de procesar el 3er Dataset y de la parte 2.

# Instrucciones
## Parte 1:
    - Se entrega un notebook para cada dataset
    - Cada notebook por completo realiza el procesamiento del dataset
## Parte 2:
    - Se entrega un notebook para el procesamiento de los datos, el cual realiza todos los pasos necesarios para producir el csv entregado en kaggle
    - En el caso de no tener descargados los recursos de nltk necesarios para la ejecución, existe una celda con los comandos para descargarlos.
  
## Dependencias:
Se necesitan los siguientes paquetes de python para ejecutar los notebooks
- category_encoders
- sklearn
- pandas
- numpy
- nltk

Los paquetes se pueden instalar usando
```bash
pip install <nombre del paquete>
```
